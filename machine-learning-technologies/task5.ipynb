{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "Made by Roman Efremov, J41325c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-01 19:39:57--  http://www.gutenberg.org/files/11/11-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000::bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 174693 (171K) [text/plain]\n",
      "Saving to: ‘11-0.txt.3’\n",
      "\n",
      "11-0.txt.3          100%[===================>] 170.60K   295KB/s    in 0.6s    \n",
      "\n",
      "2021-03-01 19:40:16 (295 KB/s) - ‘11-0.txt.3’ saved [174693/174693]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.gutenberg.org/files/11/11-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = None\n",
    "with open('11-0.txt', 'r') as file:\n",
    "    file_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Alice’s Adventures in Wonderland, by Lewis Carroll\\n\\nThis eBook is for the use of anyone anywhere in the United States and most\\nother parts of the world at no cost and with almost no restrictions\\nwhatsoever.  You may copy it, give it away or re-use it under the terms of\\nthe Project Gutenberg License included with this eBook or online at\\nwww.gutenberg.org.  If you are'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_content[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_book = re.compile('THE END').search(file_content).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = file_content[:end_of_book]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_regex = re.compile('CHAPTER [IXV]+\\.')\n",
    "chapter_matches = chapter_regex.finditer(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From book content we can see that first 12 matches are from contents section. So we drop first 12 matches and start splitting chapters from the 13-d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1437, 1447), match='CHAPTER I.'>\n",
      "<re.Match object; span=(12993, 13004), match='CHAPTER II.'>\n",
      "<re.Match object; span=(23951, 23963), match='CHAPTER III.'>\n",
      "<re.Match object; span=(33217, 33228), match='CHAPTER IV.'>\n",
      "<re.Match object; span=(47106, 47116), match='CHAPTER V.'>\n",
      "<re.Match object; span=(59122, 59133), match='CHAPTER VI.'>\n",
      "<re.Match object; span=(72971, 72983), match='CHAPTER VII.'>\n",
      "<re.Match object; span=(85680, 85693), match='CHAPTER VIII.'>\n",
      "<re.Match object; span=(99355, 99366), match='CHAPTER IX.'>\n",
      "<re.Match object; span=(111991, 112001), match='CHAPTER X.'>\n",
      "<re.Match object; span=(123407, 123418), match='CHAPTER XI.'>\n",
      "<re.Match object; span=(133799, 133811), match='CHAPTER XII.'>\n"
     ]
    }
   ],
   "source": [
    "prev_start = None\n",
    "chapters_texts = []\n",
    "\n",
    "for i, match in enumerate(chapter_matches):\n",
    "    if i < 12:\n",
    "        continue\n",
    "    print(match)\n",
    "    start = match.start()\n",
    "    if prev_start is not None:\n",
    "        chapter_text = file_content[prev_start:start]\n",
    "        chapters_texts.append(chapter_text)\n",
    "    prev_start = start\n",
    "\n",
    "chapter_text = file_content[prev_start:]\n",
    "chapters_texts.append(chapter_text)\n",
    "chapters_texts = np.array(chapters_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER I.\\nDown the Rabbit-Hole\\n\\n\\nAlice was beginning to get very tired of sitting by her sister on ',\n",
       " 'CHAPTER II.\\nThe Pool of Tears\\n\\n\\n“Curiouser and curiouser!” cried Alice (she was so much surprised, t',\n",
       " 'CHAPTER III.\\nA Caucus-Race and a Long Tale\\n\\n\\nThey were indeed a queer-looking party that assembled o',\n",
       " 'CHAPTER IV.\\nThe Rabbit Sends in a Little Bill\\n\\n\\nIt was the White Rabbit, trotting slowly back again,',\n",
       " 'CHAPTER V.\\nAdvice from a Caterpillar\\n\\n\\nThe Caterpillar and Alice looked at each other for some time ',\n",
       " 'CHAPTER VI.\\nPig and Pepper\\n\\n\\nFor a minute or two she stood looking at the house, and wondering what\\n',\n",
       " 'CHAPTER VII.\\nA Mad Tea-Party\\n\\n\\nThere was a table set out under a tree in front of the house, and the',\n",
       " 'CHAPTER VIII.\\nThe Queen’s Croquet-Ground\\n\\n\\nA large rose-tree stood near the entrance of the garden: ',\n",
       " 'CHAPTER IX.\\nThe Mock Turtle’s Story\\n\\n\\n“You can’t think how glad I am to see you again, you dear old ',\n",
       " 'CHAPTER X.\\nThe Lobster Quadrille\\n\\n\\nThe Mock Turtle sighed deeply, and drew the back of one flapper a',\n",
       " 'CHAPTER XI.\\nWho Stole the Tarts?\\n\\n\\nThe King and Queen of Hearts were seated on their throne when the',\n",
       " 'CHAPTER XII.\\nAlice’s Evidence\\n\\n\\n“Here!” cried Alice, quite forgetting in the flurry of the moment ho']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s[:100] for s in chapters_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # to lower case\n",
    "    text = text.lower()\n",
    "    # remove all except letters and '-'\n",
    "    text = re.sub('[^a-zA-Z-]+', ' ', text)\n",
    "    # remove extra spaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_chapter_texts = list(map(pre_process, chapters_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter i down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the',\n",
       " 'chapter ii the pool of tears curiouser and curiouser cried alice she was so much surprised that for ',\n",
       " 'chapter iii a caucus-race and a long tale they were indeed a queer-looking party that assembled on t',\n",
       " 'chapter iv the rabbit sends in a little bill it was the white rabbit trotting slowly back again and ',\n",
       " 'chapter v advice from a caterpillar the caterpillar and alice looked at each other for some time in ',\n",
       " 'chapter vi pig and pepper for a minute or two she stood looking at the house and wondering what to d',\n",
       " 'chapter vii a mad tea-party there was a table set out under a tree in front of the house and the mar',\n",
       " 'chapter viii the queen s croquet-ground a large rose-tree stood near the entrance of the garden the ',\n",
       " 'chapter ix the mock turtle s story you can t think how glad i am to see you again you dear old thing',\n",
       " 'chapter x the lobster quadrille the mock turtle sighed deeply and drew the back of one flapper acros',\n",
       " 'chapter xi who stole the tarts the king and queen of hearts were seated on their throne when they ar',\n",
       " 'chapter xii alice s evidence here cried alice quite forgetting in the flurry of the moment how large']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t[:100] for t in pre_processed_chapter_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['chapter', 'i', 'down', 'the', 'rabbit-hole', 'alice'],\n",
       " ['chapter', 'ii', 'the', 'pool', 'of', 'tears'],\n",
       " ['chapter', 'iii', 'a', 'caucus-race', 'and', 'a'],\n",
       " ['chapter', 'iv', 'the', 'rabbit', 'sends', 'in'],\n",
       " ['chapter', 'v', 'advice', 'from', 'a', 'caterpillar'],\n",
       " ['chapter', 'vi', 'pig', 'and', 'pepper', 'for'],\n",
       " ['chapter', 'vii', 'a', 'mad', 'tea-party', 'there'],\n",
       " ['chapter', 'viii', 'the', 'queen', 's', 'croquet-ground'],\n",
       " ['chapter', 'ix', 'the', 'mock', 'turtle', 's'],\n",
       " ['chapter', 'x', 'the', 'lobster', 'quadrille', 'the'],\n",
       " ['chapter', 'xi', 'who', 'stole', 'the', 'tarts'],\n",
       " ['chapter', 'xii', 'alice', 's', 'evidence', 'here']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokenized_chapter_texts = list(map(tokenizer.tokenize, list(pre_processed_chapter_texts)))\n",
    "[t[:6] for t in tokenized_chapter_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/merfemor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_remove_stop_words(tokens):\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return list(map(lemmatizer.lemmatize, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['chapter', 'rabbit-hole', 'alice', 'beginning', 'get', 'tired'],\n",
       " ['chapter', 'ii', 'pool', 'tear', 'curiouser', 'curiouser'],\n",
       " ['chapter', 'iii', 'caucus-race', 'long', 'tale', 'indeed'],\n",
       " ['chapter', 'iv', 'rabbit', 'sends', 'little', 'bill'],\n",
       " ['chapter', 'v', 'advice', 'caterpillar', 'caterpillar', 'alice'],\n",
       " ['chapter', 'vi', 'pig', 'pepper', 'minute', 'two'],\n",
       " ['chapter', 'vii', 'mad', 'tea-party', 'table', 'set'],\n",
       " ['chapter', 'viii', 'queen', 'croquet-ground', 'large', 'rose-tree'],\n",
       " ['chapter', 'ix', 'mock', 'turtle', 'story', 'think'],\n",
       " ['chapter', 'x', 'lobster', 'quadrille', 'mock', 'turtle'],\n",
       " ['chapter', 'xi', 'stole', 'tart', 'king', 'queen'],\n",
       " ['chapter', 'xii', 'alice', 'evidence', 'cried', 'alice']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter_text_tokens = list(map(lemmatize_remove_stop_words, tokenized_chapter_texts))\n",
    "[t[:6] for t in chapter_text_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_tf(tokens):\n",
    "    c = Counter()\n",
    "    for token in tokens:\n",
    "        c[token] += 1\n",
    "    return { k: c[k] / len(tokens) for k in dict(c) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_idf(tokens, corpus_tokens_set):\n",
    "    c = Counter()\n",
    "    unique_tokens = set(tokens)\n",
    "    for token in unique_tokens:\n",
    "        for doc in corpus_tokens_set:\n",
    "            if token in doc:\n",
    "                c[token] += 1\n",
    "    return { k: math.log(len(corpus_tokens_set) / c[k]) for k in c }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_tokens_sets = [set(t) for t in chapter_text_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_idf(tokens, corpus_tokens_set):\n",
    "    c = Counter()\n",
    "    unique_tokens = set(tokens)\n",
    "    for token in unique_tokens:\n",
    "        for doc in corpus_tokens_set:\n",
    "            if token in doc:\n",
    "                c[token] += 1\n",
    "    return { k: math.log(len(corpus_tokens_set) / c[k]) for k in c }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(tokens, corpus_tokens_set):\n",
    "    unique_tokens = set(tokens)\n",
    "    tfs = compute_tf(tokens)\n",
    "    idfs = compute_idf(tokens, corpus_tokens_set)\n",
    "    \n",
    "    result = tfs\n",
    "    for token in unique_tokens:\n",
    "        result[token] = tfs[token] * idfs[token]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idfs = [compute_tf_idf(t, docs_tokens_sets) for t in chapter_text_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 chapter top 10: bat, key, dark, poison, candle, bottle, eat, fell, marked, dinah\n",
      "2 chapter top 10: mouse, swam, pool, mabel, glove, fan, dog, cat, four, kid\n",
      "3 chapter top 10: dodo, mouse, prize, lory, thimble, dry, caucus-race, dinah, tale, bird\n",
      "4 chapter top 10: window, puppy, bill, glove, fan, bottle, chimney, mary, ann, yer\n",
      "5 chapter top 10: caterpillar, serpent, pigeon, youth, egg, father, size, hookah, green, taller\n",
      "6 chapter top 10: footman, baby, cat, mad, pig, wow, grunted, cook, duchess, livery\n",
      "7 chapter top 10: dormouse, hatter, march, hare, twinkle, clock, draw, tea, asleep, civil\n",
      "8 chapter top 10: queen, hedgehog, gardener, king, procession, executioner, five, soldier, rose-tree, cat\n",
      "9 chapter top 10: turtle, mock, moral, gryphon, duchess, queen, tortoise, school, chin, ti\n",
      "10 chapter top 10: turtle, mock, lobster, gryphon, dance, join, whiting, soo, oop, soup\n",
      "11 chapter top 10: hatter, king, court, witness, dormouse, juror, officer, tart, bread-and-butter, trumpet\n",
      "12 chapter top 10: king, jury, dream, juryman, fit, important, sister, unimportant, slate, queen\n"
     ]
    }
   ],
   "source": [
    "for i, tf_idf in enumerate(tf_idfs):\n",
    "    top10 = [k for k, v in sorted(tf_idf.items(), key=lambda item: item[1], reverse=True)][:10]\n",
    "    print(i + 1, \"chapter top 10:\", ', '.join(top10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Top 10 words in chapters according to TF-IDF metric look plausible. They really characterize each chapter's contents.\n",
    "\n",
    "Here the original chapter names:\n",
    "```\n",
    " CHAPTER I.     Down the Rabbit-Hole\n",
    " CHAPTER II.    The Pool of Tears\n",
    " CHAPTER III.   A Caucus-Race and a Long Tale\n",
    " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
    " CHAPTER V.     Advice from a Caterpillar\n",
    " CHAPTER VI.    Pig and Pepper\n",
    " CHAPTER VII.   A Mad Tea-Party\n",
    " CHAPTER VIII.  The Queen’s Croquet-Ground\n",
    " CHAPTER IX.    The Mock Turtle’s Story\n",
    " CHAPTER X.     The Lobster Quadrille\n",
    " CHAPTER XI.    Who Stole the Tarts?\n",
    " CHAPTER XII.   Alice’s Evidence\n",
    "```\n",
    "For example, in the 1st chapter's top 10 there are words like 'rabbit-hole' and 'dark', which are look simmilar to chapter name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/merfemor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.tokenize.sent_tokenize(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_with_alice = [s for s in sentences if 'Alice' in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = [pre_process(s) for s in sentences_with_alice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/merfemor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = [nltk.pos_tag(nltk.word_tokenize(s)) for s in processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in tagged for tt in t if tt[0] == 'wa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_verbs = []\n",
    "for ts in tagged:\n",
    "    for word, tag in ts:\n",
    "        if tag.startswith('VB'):\n",
    "            tagged_verbs.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = [lemmatizer.lemmatize(t) for t in tagged_verbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tagged_verbs:\n",
    "    if word_count[w] is None:\n",
    "        word_count[w] = 1\n",
    "    else:\n",
    "        word_count[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_verbs = sorted(word_count.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 258),\n",
       " ('was', 177),\n",
       " ('had', 94),\n",
       " ('be', 83),\n",
       " ('s', 66),\n",
       " ('is', 53),\n",
       " ('i', 50),\n",
       " ('know', 47),\n",
       " ('do', 45),\n",
       " ('have', 44)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_verbs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 verbs are pretty obvious and are most popular verbs. But there some strange words: 's' and 'i'.\n",
    "'s' is probably 'is', 'has' or 'was' from contracted form of some word (e.g. she's).\n",
    "'i' is just missclassification of the nltk POS-tagger. Here the example of sentence with bad classification:\n",
    "```\n",
    "[('i', 'VB'),\n",
    "  ('shall', 'MD'),\n",
    "  ('be', 'VB'),\n",
    "  ('late', 'RB'),\n",
    "  ('when', 'WRB'),\n",
    "  ('she', 'PRP'),\n",
    "  ('thought', 'VBD'),\n",
    "  ...\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
